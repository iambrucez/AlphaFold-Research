{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import dgl\n",
    "# data loading modules\n",
    "from data.reaction_dataset import ReactionDataset\n",
    "from data.load_data import load_data\n",
    "# gnn models\n",
    "from model.gcn import GCN\n",
    "from model.gat import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset created\n",
      ">> feature_list created\n",
      ">> adjM created\n",
      ">> train_val_test_idx set\n",
      ">> labels set\n",
      ">> Graph built\n"
     ]
    }
   ],
   "source": [
    "dataset = 'iYO844'\n",
    "device = torch.device('cpu')\n",
    "    \n",
    "    # 'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "data = load_data(dataset, device)\n",
    "\n",
    "node_count_by_type = dict(data['rd'].nodes['count'])\n",
    "\n",
    "features_list = data['features_list']\n",
    "num_labels = data['num_labels']\n",
    "m_fea_dim = data['m_feature_dim']\n",
    "\n",
    "labels = data['labels']\n",
    "\n",
    "g = data['g']\n",
    "\n",
    "train_idx = data['train_val_test_idx']['train_idx']\n",
    "val_idx = data['train_val_test_idx']['val_idx']\n",
    "test_idx = data['train_val_test_idx']['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node data loader\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, train_idx, sampler,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)\n",
    "\n",
    "input_nodes, output_nodes, blocks = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_nodes, output_nodes, blocks in dataloader:\n",
    "#     print(input_nodes)\n",
    "#     print(output_nodes)\n",
    "#     print(blocks)\n",
    "#     print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_count_by_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# (out_channels, kernel_size, stride, padding)\n",
    "enzyme_conv_archi = [\n",
    "    # input: 1600x1600x2\n",
    "    (2, 4, 4, 0),\n",
    "    # 400x400x2\n",
    "    (1, 4, 4, 0),\n",
    "    # 100x100x1\n",
    "    (1, 4, 4, 0),\n",
    "    # 25x25x1\n",
    "]\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "        \n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 m_fea_dim,\n",
    "                 dropout,\n",
    "                 num_hidden,\n",
    "                 num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.m_fea_dim = m_fea_dim\n",
    "\n",
    "        # Enzyme feature\n",
    "        ## Conv2d layers for enzyme logits\n",
    "        self.conv2d_layers = nn.ModuleList()\n",
    "\n",
    "        in_channels = 2\n",
    "        for x in enzyme_conv_archi:\n",
    "            self.conv2d_layers.append(Conv2dBlock(in_channels, x[0], kernel_size=x[1], stride=x[2], padding=x[3],))\n",
    "            in_channels = x[0]\n",
    "            \n",
    "        self.conv2d_layers.append(nn.Flatten(start_dim=1))\n",
    "        self.conv2d_layers.append(nn.Linear(625, 64))\n",
    "        self.dropout_conv2d = nn.Dropout(dropout)\n",
    "\n",
    "        ## Linear layers for enzyme single\n",
    "        self.single_linear = nn.Linear(1600, 64)\n",
    "\n",
    "        # fc layers: to make the features of all the nodes become the same dimension  \n",
    "        in_dims = [257, 257]\n",
    "        self.fc_list = nn.ModuleList(\n",
    "            [nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n",
    "        for fc in self.fc_list:\n",
    "            nn.init.xavier_normal_(fc.weight, gain=1.414)\n",
    "\n",
    "        # GC layers \n",
    "        self.GClayers = nn.ModuleList() \n",
    "        for i in range(num_layers-1):\n",
    "            self.GClayers.append(\n",
    "                GraphConv(num_hidden, num_hidden, activation=F.elu))\n",
    "        ## output layer\n",
    "        self.GClayers.append(GraphConv(num_hidden, num_labels))\n",
    "        self.dropout_GC = nn.Dropout(p=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, node_count_by_type, blocks, features_list):\n",
    "        nodes_to_train = blocks[0].srcdata['_ID'].tolist()\n",
    "\n",
    "        e_nodes = [n for n in nodes_to_train if n < node_count_by_type[0]]\n",
    "        m_nodes = [n for n in nodes_to_train if n >= node_count_by_type[0]]\n",
    "\n",
    "        # 1. Enzyme feature\n",
    "        ## 1.1 logits tensor (input [#e_nodes, 1600, 1600, 2])]))\n",
    "        e_feature_logits = None\n",
    "        for e_node in e_nodes:\n",
    "            with open('../datasets/iYO844/logits/{}.npy'.format(e_node), 'rb') as f:\n",
    "                logits = torch.unsqueeze(torch.from_numpy(np.load(f)), 0)\n",
    "            if e_feature_logits == None:\n",
    "                e_feature_logits = logits\n",
    "            else:\n",
    "                e_feature_logits = torch.cat([e_feature_logits, logits])\n",
    "\n",
    "        e_feature_logits = e_feature_logits.permute(0, 3, 1, 2)\n",
    "        for i, layer in enumerate(self.conv2d_layers):\n",
    "            e_feature_logits = self.dropout_conv2d(e_feature_logits)\n",
    "            e_feature_logits = layer(e_feature_logits)\n",
    "        # After: e_feature_logits: [#e_nodes, 64]\n",
    "        print(f'e_feature_logits: {e_feature_logits.shape}')\n",
    "\n",
    "        # 1.2 single representation vector\n",
    "        e_feature_single = features_list[0][e_nodes]\n",
    "        e_feature_single = self.single_linear(e_feature_single)\n",
    "        print(f'e_feature_single: {e_feature_single.shape}')\n",
    "\n",
    "        ## 1.3 concatenate logits and single\n",
    "        e_feature = torch.cat((e_feature_logits, e_feature_single), 1)\n",
    "        e_dim = e_feature.shape[1]\n",
    "        # After: e_feature: [384, 128]\n",
    "        print(f'e_feature: {e_feature.shape}')\n",
    "\n",
    "\n",
    "        # 2. Pad the features of all the nodes and make them the same dimension\n",
    "        m_feature = features_list[1][[idx-node_count_by_type[0] for idx in m_nodes]]\n",
    "        e_feature = torch.cat((e_feature, torch.zeros((e_feature.shape[0], self.m_fea_dim))), 1)  # [384, e_dim+129] i.e. [616, 257]\n",
    "        print(f'e_feature: {e_feature.shape}')\n",
    "        m_feature = torch.cat((torch.zeros((m_feature.shape[0], e_dim)), m_feature), 1)  # [616, 128+m_dim] i.e. [616, 257]\n",
    "        print(f'm_feature: {m_feature.shape}')\n",
    "        features_list = [e_feature, m_feature]\n",
    "        h = []\n",
    "        for fc, feature in zip(self.fc_list, features_list):\n",
    "            h.append(fc(feature))\n",
    "        h = torch.cat(h, 0) # [1000, hidden_dim]\n",
    "        print(f'h: {h.shape}')\n",
    "\n",
    "        # 3. GC layers\n",
    "        for i, layer in enumerate(self.GClayers):\n",
    "            h = self.dropout_GC(h)\n",
    "            h = layer(blocks[i], h)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def get_e_feature_logits(self, node_count_by_type, nodes_to_infer):\n",
    "        e_nodes = [n for n in nodes_to_infer if n < node_count_by_type[0]]\n",
    "\n",
    "        # 1. Enzyme feature\n",
    "        ## 1.1 logits tensor (input [#e_nodes, 1600, 1600, 2])]))\n",
    "        e_feature_logits = None\n",
    "        for e_node in e_nodes:\n",
    "            with open('../datasets/iYO844/logits/{}.npy'.format(e_node), 'rb') as f:\n",
    "                logits = torch.unsqueeze(torch.from_numpy(np.load(f)), 0)\n",
    "            if e_feature_logits == None:\n",
    "                e_feature_logits = logits\n",
    "            else:\n",
    "                e_feature_logits = torch.cat([e_feature_logits, logits])\n",
    "\n",
    "        e_feature_logits = e_feature_logits.permute(0, 3, 1, 2)\n",
    "        for i, layer in enumerate(self.conv2d_layers):\n",
    "            e_feature_logits = self.dropout_conv2d(e_feature_logits)\n",
    "            e_feature_logits = layer(e_feature_logits)\n",
    "        # After: e_feature_logits: [#e_nodes, 64]\n",
    "        print(f'e_feature_logits: {e_feature_logits.shape}')\n",
    "\n",
    "        return e_feature_logits\n",
    "\n",
    "\n",
    "    def inference(self, features_list):\n",
    "        # 1. Enzyme feature\n",
    "        ## 1.1 logits tensor (input [384, 1600, 1600, 2])]))\n",
    "        e_feature_logits = None\n",
    "        for pointer in range(node_count_by_type[0] // 100 + 1):\n",
    "            nodes_to_infer = list(range(pointer*100, min(pointer*100+100, node_count_by_type[0])))\n",
    "            e_feature_logits_t = self.get_e_feature_logits(node_count_by_type, nodes_to_infer)\n",
    "\n",
    "            if e_feature_logits == None:\n",
    "                e_feature_logits = e_feature_logits_t\n",
    "            else:\n",
    "                e_feature_logits = torch.cat([e_feature_logits, e_feature_logits_t])\n",
    "\n",
    "        ## 1.2 single representation vector\n",
    "        e_feature_single = features_list[0]\n",
    "        e_feature_single = self.single_linear(e_feature_single)\n",
    "        # After: e_feature_single: [384, 64]\n",
    "        print(f'e_feature_single: {e_feature_single.shape}')\n",
    "\n",
    "        ## 1.3 concatenate logits and single\n",
    "        e_feature = torch.cat((e_feature_logits, e_feature_single), 1)\n",
    "        e_dim = e_feature.shape[1]\n",
    "        # After: e_feature: [384, 128]\n",
    "        print(f'e_feature: {e_feature.shape}')\n",
    "\n",
    "        # 2. Pad the features of all the nodes and make them the same dimension\n",
    "        m_feature = features_list[1]\n",
    "        \n",
    "        e_feature = torch.cat((e_feature, torch.zeros((e_feature.shape[0], self.m_fea_dim))), 1)  # [384, e_dim+129] i.e. [616, 257]\n",
    "        # print(f'e_feature: {e_feature.shape}')\n",
    "        \n",
    "        m_feature = torch.cat((torch.zeros((m_feature.shape[0], e_dim)), m_feature), 1)  # [616, 128+m_fea_dim] i.e. [616, 257]\n",
    "        # print(f'm_feature: {m_feature.shape}')\n",
    "        \n",
    "        features_list = [e_feature, m_feature]\n",
    "        \n",
    "        h = []\n",
    "        for fc, feature in zip(self.fc_list, features_list):\n",
    "            h.append(fc(feature))\n",
    "        h = torch.cat(h, 0) # [1000, hidden_dim]\n",
    "        \n",
    "        # print(f'h: {h.shape}')\n",
    "        \n",
    "        # 3. GC layers\n",
    "        for i, layer in enumerate(self.GClayers):\n",
    "            h = self.dropout_GC(h)\n",
    "            h = layer(self.g, h)\n",
    "            \n",
    "        # After: h: [1000, num_labels]\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GCN(g, m_fea_dim, 0.3, 64, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_feature_logits: torch.Size([100, 64])\n",
      "e_feature_logits: torch.Size([100, 64])\n",
      "e_feature_logits: torch.Size([100, 64])\n",
      "e_feature_logits: torch.Size([84, 64])\n",
      "e_feature_single: torch.Size([384, 64])\n",
      "e_feature: torch.Size([384, 128])\n"
     ]
    }
   ],
   "source": [
    "h_i = net.inference(g, device='cpu', features_list=features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_feature_logits: torch.Size([52, 64])\n",
      "e_feature_single: torch.Size([52, 64])\n",
      "e_feature: torch.Size([52, 128])\n",
      "e_feature: torch.Size([52, 257])\n",
      "m_feature: torch.Size([28, 257])\n",
      "h: torch.Size([80, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5942],\n",
       "        [-0.1212],\n",
       "        [-0.5024],\n",
       "        [-0.1555],\n",
       "        [-0.0362],\n",
       "        [ 0.0756],\n",
       "        [ 1.3392],\n",
       "        [ 0.3113],\n",
       "        [-0.2947],\n",
       "        [ 0.7500]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = net(node_count_by_type, blocks, features_list)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7173,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.4012,\n",
       "         0.0000, -1.2730,  0.0000,  0.0000,  0.0000,  0.0000,  2.7363,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.1812,\n",
       "         0.0000,  0.0000,  2.3979,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "        -0.9933,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8283,\n",
       "        -6.9078,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         1.6094,  0.0000,  0.0000,  3.5263,  0.0000,  0.4824,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, -4.4228,  0.0000, -4.7217,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9957, -0.1450,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000, -0.1054,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6043,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  3.2958,  0.0000,  0.0000,  0.0000,  0.5306,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.0794,  0.0000, -0.8468,\n",
       "         0.0000,  0.0000,  0.0000,  2.1633,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.7031,  0.0000,  0.0000,  0.0000, -0.2614,  0.0000,  0.6931,\n",
       "         0.0000, -2.8302,  0.0000, -3.4112,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000, -3.3141, -2.2340,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  1.6094, -2.1716,  0.0000,  0.0000, -1.1536,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4882,  0.0000,\n",
       "        -1.9661,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         1.0986,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, -8.0164,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000, -1.6437, -5.2214,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000, -1.4727,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "        -1.4271,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         3.9703,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.9722,  0.0000,\n",
       "         1.3653,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -5.2159,  0.3853,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  1.2384, -3.7297, -1.5803,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, -3.0366,  0.0000,  0.3365,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9021, -2.5639,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.6889,  2.4159, -1.3662,\n",
       "        -5.4538,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3857,  0.0000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99b8c61c0ac2b35f6b37e4c1a47d487000a055a90bb2cd41e87c4adb32f442af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('graphNN_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
